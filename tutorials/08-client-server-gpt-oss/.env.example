# Local LLM Configuration
LLM_PROVIDER=openai  # Options: openai, ollama, transformers
MODEL_NAME=gpt-oss:20b  # For OpenAI: lightweight and fast model

# Ollama Configuration (if using Ollama)
OLLAMA_HOST=http://localhost:11434
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama

# Server Configuration
STORAGE_MCP_SERVER_URL=http://localhost:8000/sse
